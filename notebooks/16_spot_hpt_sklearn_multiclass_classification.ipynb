{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Scikit-learn Hyperparameter Tuning With spotPython - A Tutorial for Classification: RandomForestClassifier\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIME = 300 # Time in minutes. Counter starts, after the initial design is evaluated. So, runtime can be larger.\n",
    "INIT_SIZE = 20 # Initial number of designs to evaluate, before the surrogate is build.\n",
    "ORIGINAL = False # If True, the original data is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.tz import tzlocal\n",
    "start_time = datetime.now(tzlocal())\n",
    "HOSTNAME = socket.gethostname().split(\".\")[0]\n",
    "experiment_name = '16-sklearnRandomForestClassifier' + \"_\" + HOSTNAME + \"_\" + str(MAX_TIME) + \"min_\" + str(INIT_SIZE) + \"init_\" + str(start_time).split(\".\", 1)[0].replace(' ', '_')\n",
    "experiment_name = experiment_name.replace(':', '-')\n",
    "print(experiment_name)\n",
    "if not os.path.exists('./figures'):\n",
    "    os.makedirs('./figures')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: Sequential Parameter Optimization\n",
    "## Hyperparameter Tuning: sklearn RandomForestClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This notebook exemplifies hyperparameter tuning with SPOT (spotPython).\n",
    "* The hyperparameter software SPOT was developed in R (statistical programming language), see Open Access book \"Hyperparameter Tuning for Machine and Deep Learning with R - A Practical Guide\", available here: [https://link.springer.com/book/10.1007/978-981-19-5170-1](https://link.springer.com/book/10.1007/978-981-19-5170-1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list | grep  \"spot[RiverPython]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install --upgrade build\n",
    "# !{sys.executable} -m pip install --upgrade --force-reinstall spotPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "import warnings\n",
    "import numpy as np\n",
    "from math import inf\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder , MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import make_pipeline , Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, roc_curve, roc_auc_score, log_loss, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from spotPython.spot import spot\n",
    "from spotPython.hyperparameters.values import (\n",
    "    add_core_model_to_fun_control,\n",
    "    assign_values,\n",
    "    convert_keys,\n",
    "    get_bound_values,\n",
    "    get_default_hyperparameters_for_core_model,\n",
    "    get_default_values,\n",
    "    get_default_hyperparameters_as_array,\n",
    "    get_dict_with_levels_and_types,\n",
    "    get_values_from_dict,\n",
    "    get_var_name,\n",
    "    get_var_type,\n",
    "    iterate_dict_values,\n",
    "    modify_hyper_parameter_levels,\n",
    "    modify_hyper_parameter_bounds,\n",
    "    replace_levels_with_positions,\n",
    "    return_conf_list_from_var_dict,\n",
    "    get_one_core_model_from_X,\n",
    "    transform_hyper_parameter_values,\n",
    "    get_dict_with_levels_and_types,\n",
    "    convert_keys,\n",
    "    iterate_dict_values,\n",
    "    get_one_sklearn_model_from_X\n",
    ")\n",
    "\n",
    "from spotPython.utils.convert import class_for_name\n",
    "from spotPython.utils.eda import (\n",
    "    get_stars,\n",
    "    gen_design_table)\n",
    "from spotPython.utils.transform import transform_hyper_parameter_values\n",
    "from spotPython.utils.convert import get_Xy_from_df\n",
    "from spotPython.plot.validation import plot_cv_predictions, plot_roc, plot_confusion_matrix\n",
    "from spotPython.utils.init import fun_control_init\n",
    "\n",
    "from spotPython.data.sklearn_hyper_dict import SklearnHyperDict\n",
    "from spotPython.fun.hypersklearn import HyperSklearn\n",
    "from spotPython.utils.metrics import mapk_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialization of the Empty `fun_control` Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control = fun_control_init()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data: Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Borne Disease Prediction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ORIGINAL == True:\n",
    "    train_df = pd.read_csv('./data/VBDP/trainn.csv')\n",
    "    test_df = pd.read_csv('./data/VBDP/testt.csv')\n",
    "else:\n",
    "    train_df = pd.read_csv('./data/VBDP/train.csv')\n",
    "    # remove the id column\n",
    "    train_df = train_df.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = train_df.shape[0]\n",
    "n_features = train_df.shape[1] - 1\n",
    "target_column = \"prognosis\"\n",
    "# Encoder our prognosis labels as integers for easier decoding later\n",
    "enc = OrdinalEncoder()\n",
    "train_df[target_column] = enc.fit_transform(train_df[[target_column]])\n",
    "train_df.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full data set `train_df` 64 features. The target column is labeled as `prognosis`.\n",
    "\n",
    "## Holdout Train and Test Data\n",
    "\n",
    "We split out a hold-out test set (25% of the data) so we can calculate an example MAP@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_df.drop(target_column, axis=1), train_df[target_column],\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify=train_df[target_column])\n",
    "train = pd.DataFrame(np.hstack((X_train, np.array(y_train).reshape(-1, 1))))\n",
    "test = pd.DataFrame(np.hstack((X_test, np.array(y_test).reshape(-1, 1))))\n",
    "train.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
    "test.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the dataset to the fun_control\n",
    "fun_control.update({\"data\": train_df, # full dataset,\n",
    "               \"train\": train,\n",
    "               \"test\": test,\n",
    "               \"n_samples\": n_samples,\n",
    "               \"target_column\": target_column})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Specification of the Preprocessing Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocesssing can be very simple, e.g., you can ignore it. Then you would choose the `prep_model` \"None\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_model = None\n",
    "fun_control.update({\"prep_model\": prep_model})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A default approach for numerical data is the `StandardScaler` (mean 0, variance 1).  This can be selected as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_model = StandardScaler()\n",
    "# fun_control.update({\"prep_model\": prep_model})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more complicated pre-processing steps are possible, e.g., the follwing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_columns = []\n",
    "# one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "# prep_model = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             (\"categorical\", one_hot_encoder, categorical_columns),\n",
    "#         ],\n",
    "#         remainder=StandardScaler(),\n",
    "#     )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select `algorithm` and `core_model_hyper_dict`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selection of the algorithm (ML model) that should be tuned is done by specifying the its name from the `sklearn` implementation.  For example, the `SVC` support vector machine classifier is selected as follows:\n",
    "\n",
    "`fun_control = add_core_model_to_fun_control(SVC, fun_control, SklearnHyperDict)`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other core_models are, e.g.,:\n",
    "\n",
    "* RidgeCV\n",
    "* GradientBoostingRegressor\n",
    "* ElasticNet\n",
    "* RandomForestClassifier\n",
    "* LogisticRegression\n",
    "* KNeighborsClassifier\n",
    "* RandomForestClassifier\n",
    "* GradientBoostingClassifier\n",
    "* HistGradientBoostingClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `RandomForestClassifier` classifier in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control = add_core_model_to_fun_control(core_model=RandomForestClassifier,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=SklearnHyperDict,\n",
    "                              filename=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `fun_control` has the information from the JSON file. The available hyperparameters are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*fun_control[\"core_model_hyper_dict\"].keys(), sep=\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modify `hyper_dict` Hyperparameters for the Selected Algorithm aka `core_model`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify hyperparameter of type factor\n",
    "\n",
    "Factors can be modified with the `modify_hyper_parameter_levels` function.  For example, to exclude the `sigmoid` kernel from the tuning, the `kernel` hyperparameter of the `SVC` model can be modified as follows:\n",
    "\n",
    "`fun_control = modify_hyper_parameter_levels(fun_control, \"kernel\", [\"linear\", \"rbf\"])`\n",
    "\n",
    "The new setting can be controlled via:\n",
    "\n",
    "`fun_control[\"core_model_hyper_dict\"][\"kernel\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost:\n",
    "# fun_control = modify_hyper_parameter_levels(fun_control, \"loss\", [\"log_loss\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify hyperparameter of type numeric and integer (boolean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numeric and boolean values can be modified using the `modify_hyper_parameter_bounds` method.  For example, to change the `tol` hyperparameter of the `SVC` model to the interval [1e-3, 1e-2], the following code can be used:\n",
    "\n",
    "`fun_control = modify_hyper_parameter_bounds(fun_control, \"tol\", bounds=[1e-3, 1e-2])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fun_control = modify_hyper_parameter_bounds(fun_control, \"tol\", bounds=[1e-3, 1e-2])\n",
    "# fun_control = modify_hyper_parameter_bounds(fun_control, \"min_samples_split\", bounds=[3, 20])\n",
    "# fun_control = modify_hyper_parameter_bounds(fun_control, \"dual\", bounds=[0, 0])\n",
    "# fun_control = modify_hyper_parameter_bounds(fun_control, \"probability\", bounds=[1, 1])\n",
    "# fun_control[\"core_model_hyper_dict\"][\"tol\"]\n",
    "# fun_control = modify_hyper_parameter_bounds(fun_control, \"min_samples_leaf\", bounds=[1, 25])\n",
    "# fun_control = modify_hyper_parameter_bounds(fun_control, \"n_estimators\", bounds=[5, 10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "### Note: Out-of-bag Estimation\n",
    "\n",
    "Since `oob_score` requires the `bootstrap` hyperparameter to `True`, we set the `oob_score` parameter to `False`. The `oob_score` is later discussed in @sec-oob-score.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control = modify_hyper_parameter_bounds(fun_control, \"bootstrap\", bounds=[0, 1])\n",
    "fun_control = modify_hyper_parameter_bounds(fun_control, \"oob_score\", bounds=[0, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Selection of the Objective: Metric and Loss Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Machine learning models are optimized with respect to a metric, for example, the `accuracy` function.\n",
    "* Deep learning, e.g., neural networks are optimized with respect to a loss function, for example, the `cross_entropy` function and evaluated with respect to a metric, for example, the `accuracy` function.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The loss function, that is usually used in deep learning for optimizing the weights of the net, is stored in the `fun_control` dictionary as `\"loss_function\"`.\n",
    "\n",
    "\n",
    "::: {.callout-warning}\n",
    "#### Use of the Key `\"criterion\"` is deprecated as of `v.0.1.5`\n",
    "Starting with version `v.0.1.5` `spotPython` uses the key `\"loss_function\"` for loss functions instead of `\"criterion\"`.\n",
    ":::\n",
    "\n",
    "### Metric Function\n",
    "\n",
    "There are two different types of metrics in `spotPython`:\n",
    "\n",
    "1. `\"metric_river\"` is used for the river based evaluation via `eval_oml_iter_progressive`.\n",
    "2. `\"metric_sklearn\"` is used for the sklearn based evaluation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider multi-class classification metrics, e.g., `mapk_score` and `top_k_accuracy_score`.\n",
    "\n",
    "::: {.callout-note}\n",
    "#### Predict Probabilities\n",
    "In this multi-class classification example the machine learning algorithm should return the probabilities of the specific classes (`\"predict_proba\"`) instead  of the  predicted values.\n",
    ":::\n",
    "\n",
    "We set `\"predict_proba\"` to `True` in the `fun_control` dictionary.\n",
    "\n",
    "#### The MAPK Metric\n",
    "\n",
    "To select the MAPK metric, the following two entries can be added to the `fun_control` dictionary:\n",
    "\n",
    "`\"metric_sklearn\": mapk_score\"`\n",
    "\n",
    "`\"metric_params\": {\"k\": 3}`.\n",
    "\n",
    "#### Other Metrics\n",
    "\n",
    "Alternatively, other metrics for multi-class classification can be used, e.g.,:\n",
    "* top_k_accuracy_score or\n",
    "* roc_auc_score\n",
    "\n",
    "The metric `roc_auc_score` requires the parameter `\"multi_class\"`, e.g., \n",
    "\n",
    "`\"multi_class\": \"ovr\"`.  \n",
    "\n",
    "This is set in the `fun_control` dictionary.\n",
    "\n",
    "::: {.callout-note}\n",
    "#### Weights\n",
    "\n",
    "`spotPython` performs a minimization, therefore, metrics that should be maximized have to be multiplied by -1.  This is done by setting `\"weights\"` to `-1`.\n",
    "\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The complete setup for the metric in our example is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control.update({\n",
    "               \"weights\": -1,\n",
    "               \"metric_sklearn\": mapk_score,\n",
    "               \"predict_proba\": True,\n",
    "               \"metric_params\": {\"k\": 3},\n",
    "               })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Hold-out Data\n",
    "\n",
    "* The default method for computing the performance is `\"eval_holdout\"`.\n",
    "* Alternatively, cross-validation can be used for every machine learning model.\n",
    "* Specifically for RandomForests, the OOB-score can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control.update({\n",
    "    \"eval\": \"train_hold_out\",\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOB Score {#sec-oob-score}\n",
    "\n",
    "Using the OOB-Score is a very efficient way to estimate the performance of a random forest classifier.  The OOB-Score is calculated on the training data and does not require a hold-out test set.  If the OOB-Score is used, the key \"eval\" in the `fun_control` dictionary should be set to `\"oob_score\"` as shown below.\n",
    "\n",
    ":::{.callout-note}\n",
    "#### OOB-Score\n",
    "In addition to setting the key `\"eval\"` in the `fun_control` dictionary to `\"oob_score\"`, the keys `\"oob_score\"` and `\"bootstrap\"` have to be set to `True`, because the OOB-Score requires the bootstrap method.\n",
    ":::\n",
    "\n",
    "* Uncomment the following lines to use the OOB-Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control.update({\n",
    "    \"eval\": \"eval_oob_score\",\n",
    "})\n",
    "fun_control = modify_hyper_parameter_bounds(fun_control, \"bootstrap\", bounds=[1, 1])\n",
    "fun_control = modify_hyper_parameter_bounds(fun_control, \"oob_score\", bounds=[1, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the OOB-score, the classical cross validation can be used.  The number of folds is set by the key `\"k_folds\"`.  For example, to use 5-fold cross validation, the key `\"k_folds\"` is set to `5`.\n",
    "Uncomment the following line to use cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fun_control.update({\n",
    "#      \"eval\": \"train_cv\",\n",
    "#      \"k_folds\": 10,\n",
    "# })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calling the SPOT Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the SPOT Parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get types and variable names as well as lower and upper bounds for the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_type = get_var_type(fun_control)\n",
    "var_name = get_var_name(fun_control)\n",
    "fun_control.update({\"var_type\": var_type,\n",
    "                    \"var_name\": var_name})\n",
    "\n",
    "lower = get_bound_values(fun_control, \"lower\")\n",
    "upper = get_bound_values(fun_control, \"upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen_design_table(fun_control))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the `Spot` Optimizer\n",
    "\n",
    "* Run SPOT for approx. x mins (`max_time`).\n",
    "* Note: the run takes longer, because the evaluation time of initial design (here: `initi_size`, 20 points) is not considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_dict=SklearnHyperDict().load()\n",
    "X_start = get_default_hyperparameters_as_array(fun_control, hyper_dict)\n",
    "X_start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interface from the sklearn setting to SPOT is provided by the function `fun_sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun = HyperSklearn(seed=123, log_level=50).fun_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner = spot.Spot(fun=fun,\n",
    "                   lower = lower,\n",
    "                   upper = upper,\n",
    "                   fun_evals = inf,\n",
    "                   fun_repeats = 1,\n",
    "                   max_time = MAX_TIME,\n",
    "                   noise = False,\n",
    "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                   var_type = var_type,\n",
    "                   var_name = var_name,\n",
    "                   infill_criterion = \"y\",\n",
    "                   n_points = 1,\n",
    "                   seed=123,\n",
    "                   log_level = 50,\n",
    "                   show_models= False,\n",
    "                   show_progress= True,\n",
    "                   fun_control = fun_control,\n",
    "                   design_control={\"init_size\": INIT_SIZE,\n",
    "                                   \"repeats\": 1},\n",
    "                   surrogate_control={\"noise\": True,\n",
    "                                      \"cod_type\": \"norm\",\n",
    "                                      \"min_theta\": -4,\n",
    "                                      \"max_theta\": 3,\n",
    "                                      \"n_theta\": len(var_name),\n",
    "                                      \"model_optimizer\": differential_evolution,\n",
    "                                      \"model_fun_evals\": 10_000,\n",
    "                                      \"log_level\": 50\n",
    "                                      })\n",
    "spot_tuner.run(X_start=X_start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Show the Progress of the hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_progress(log_y=False, filename=\"../Figures.d/\" + experiment_name+\"_progress.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* Print the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_importance(threshold=1.0, filename=\"../Figures.d/\" + experiment_name+\"_importance.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Default Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_default = get_default_values(fun_control)\n",
    "values_default = transform_hyper_parameter_values(fun_control=fun_control, hyper_parameter_values=values_default)\n",
    "values_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_default = make_pipeline(fun_control[\"prep_model\"], fun_control[\"core_model\"](**values_default))\n",
    "model_default"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get SPOT Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_dict = assign_values(X, fun_control[\"var_name\"])\n",
    "return_conf_list_from_var_dict(var_dict=v_dict, fun_control=fun_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spot = get_one_sklearn_model_from_X(X, fun_control)\n",
    "model_spot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate SPOT Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fetch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_Xy_from_df(fun_control[\"train\"], fun_control[\"target_column\"])\n",
    "X_test, y_test = get_Xy_from_df(fun_control[\"test\"], fun_control[\"target_column\"])\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit the model with the tuned hyperparameters. This gives one result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spot.fit(X_train, y_train)\n",
    "y_pred = model_spot.predict_proba(X_test)\n",
    "res = mapk_score(y_true=y_test, y_pred=y_pred, k=3)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_eval(n, model):\n",
    "    res_values = []\n",
    "    for i in range(n):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict_proba(X_test)\n",
    "        res = mapk_score(y_true=y_test, y_pred=y_pred, k=3)\n",
    "        res_values.append(res)\n",
    "    mean_res = np.mean(res_values)\n",
    "    print(f\"mean_res: {mean_res}\")\n",
    "    std_res = np.std(res_values)\n",
    "    print(f\"std_res: {std_res}\")\n",
    "    min_res = np.min(res_values)\n",
    "    print(f\"min_res: {min_res}\")\n",
    "    max_res = np.max(res_values)\n",
    "    print(f\"max_res: {max_res}\")\n",
    "    median_res = np.median(res_values)\n",
    "    print(f\"median_res: {median_res}\")\n",
    "    return mean_res, std_res, min_res, max_res, median_res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Non-deterministic Results\n",
    "\n",
    "* Because the model is non-determinstic, we perform $n=30$ runs and calculate the mean and standard deviation of the performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = repeated_eval(30, model_spot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalution of the Default Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_default.fit(X_train, y_train)[\"randomforestclassifier\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One evaluation of the default hyperparameters is performed on the hold-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_default.predict_proba(X_test)\n",
    "mapk_score(y_true=y_test, y_pred=y_pred, k=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since one single evaluation is not meaningful, we perform, similar to the evaluation of the SPOT results,  $n=30$ runs of the default setting and and calculate the mean and standard deviation of the performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = repeated_eval(30, model_default)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot: Compare Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model_default, fun_control, title = \"Default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model_spot, fun_control, title=\"SPOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(spot_tuner.y), max(spot_tuner.y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validated Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotPython.sklearn.traintest import evaluate_cv\n",
    "fun_control.update({\n",
    "     \"eval\": \"train_cv\",\n",
    "     \"k_folds\": 20,\n",
    "})\n",
    "evaluate_cv(model=model_spot, fun_control=fun_control, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control.update({\n",
    "     \"eval\": \"test_cv\",\n",
    "     \"k_folds\": 10,\n",
    "})\n",
    "evaluate_cv(model=model_spot, fun_control=fun_control, verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is the evaluation that will be used in the comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control.update({\n",
    "     \"eval\": \"data_cv\",\n",
    "     \"k_folds\": 10,\n",
    "})\n",
    "evaluate_cv(model=model_spot, fun_control=fun_control, verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Hyperparameter Contour Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For productive use, you might want to select:\n",
    "  * `min_z=min(spot_tuner.y)` and\n",
    "  * `max_z = max(spot_tuner.y)`\n",
    "* These settings are not so colorful as visualizations that use `None` for the ranges, but give better insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_important_hyperparameter_contour(threshold=0.025)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Coordinates Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.parallel_plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot all Combinations of Hyperparameters\n",
    "\n",
    "* Warning: this may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_ALL = False\n",
    "if PLOT_ALL:\n",
    "    n = spot_tuner.k\n",
    "    for i in range(n-1):\n",
    "        for j in range(i+1, n):\n",
    "            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotCondaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "81c77de872def749acd68d9955e19f0df6803301f4c1f66c3444af66334112ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
