{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: true\n",
        "  echo: true\n",
        "  warning: false\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HPT: PyTorch With VBDP {#sec-vbdp}\n",
        "\n",
        "In this tutorial, we will show how `spotPython` can be integrated into the `PyTorch`\n",
        "training workflow for a classifiaction task.\n",
        "\n",
        "::: {.callout-caution}\n",
        "### Caution: Data must be downloaded manually\n",
        "\n",
        "* Ensure that the correspondiing data is available as `./data/VBDP/train.csv`.\n",
        "\n",
        ":::\n",
        "\n",
        "This document refers to the following software versions:\n",
        "\n",
        "- ``python``: 3.10.10\n",
        "- ``torch``: 2.0.1\n",
        "- ``torchvision``: 0.15.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spotPython                 0.2.41\n",
            "spotRiver                  0.0.93\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip list | grep  \"spot[RiverPython]\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`spotPython` can be installed via pip. Alternatively, the source code can be downloaded from gitHub: [https://github.com/sequential-parameter-optimization/spotPython](https://github.com/sequential-parameter-optimization/spotPython).\n",
        "\n",
        "```{raw}\n",
        "!pip install spotPython\n",
        "```\n",
        "\n",
        "* Uncomment the following lines if you want to for (re-)installation the latest version of `spotPython` from gitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import sys\n",
        "# !{sys.executable} -m pip install --upgrade build\n",
        "# !{sys.executable} -m pip install --upgrade --force-reinstall spotPython"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup {#sec-setup-25}\n",
        "\n",
        "Before we consider the detailed experimental setup, we select the parameters that affect run time, initial design size and the device that is used.\n",
        "\n",
        "::: {.callout-caution}\n",
        "### Caution: Run time and initial design size should be increased for real experiments\n",
        "\n",
        "* MAX_TIME is set to one minute for demonstration purposes. For real experiments, this should be increased to at least 1 hour.\n",
        "* INIT_SIZE is set to 5 for demonstration purposes. For real experiments, this should be increased to at least 10.\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.callout-note}\n",
        "### Note: Device selection\n",
        "\n",
        "* The device can be selected by setting the variable `DEVICE`.\n",
        "* Since we are using a simple neural net, the setting `\"cpu\"` is preferred (on Mac).\n",
        "* If you have a GPU, you can use `\"cuda:0\"` instead.\n",
        "* If DEVICE is set to `None`, `spotPython` will automatically select the device.\n",
        "  * This might result in `\"mps\"` on Macs, which is not the best choice for simple neural nets.\n",
        "\n",
        ":::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_TIME = 1\n",
        "INIT_SIZE = 5\n",
        "DEVICE = None, #\"cpu\" # \"cuda:0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None,)\n"
          ]
        }
      ],
      "source": [
        "from spotPython.utils.device import getDevice\n",
        "DEVICE = getDevice(DEVICE)\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30-light_p040025_1min_5init_2023-06-25_20-53-30\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import copy\n",
        "import socket\n",
        "from datetime import datetime\n",
        "from dateutil.tz import tzlocal\n",
        "start_time = datetime.now(tzlocal())\n",
        "HOSTNAME = socket.gethostname().split(\".\")[0]\n",
        "experiment_name = '30-light' + \"_\" + HOSTNAME + \"_\" + str(MAX_TIME) + \"min_\" + str(INIT_SIZE) + \"init_\" + str(start_time).split(\".\", 1)[0].replace(' ', '_')\n",
        "experiment_name = experiment_name.replace(':', '-')\n",
        "print(experiment_name)\n",
        "if not os.path.exists('./figures'):\n",
        "    os.makedirs('./figures')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Initialization of the `fun_control` Dictionary\n",
        "\n",
        ":::{.callout-caution}\n",
        "### Caution: Tensorboard does not work under Windows\n",
        "* Since tensorboard does not work under Windows, we recommend setting the parameter `tensorboard_path` to `None` if you are working under Windows.\n",
        ":::\n",
        "\n",
        "`spotPython` uses a Python dictionary for storing the information required for the hyperparameter tuning process, which was described in @sec-initialization-fun-control-14, see [Initialization of the fun_control Dictionary](https://sequential-parameter-optimization.github.io/spotPython/14_spot_ray_hpt_torch_cifar10.html#sec-initialization-fun-control-14) in the documentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.utils.init import fun_control_init\n",
        "fun_control = fun_control_init(task=\"classification\",\n",
        "    tensorboard_path=\"./runs/\" + experiment_name,\n",
        "    num_workers=10,\n",
        "    device=DEVICE)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: PyTorch Data Loading {#sec-data-loading-25}\n",
        "\n",
        "### 1. Load VBDP Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64])\n",
            "tensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from spotPython.light.csvdataset import CSVDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Create an instance of CSVDataset\n",
        "dataset = CSVDataset(csv_file=\"./data/VBDP/train.csv\", train=True)\n",
        "# show the dimensions of the dataset\n",
        "print(dataset[0][0].shape)\n",
        "# show the first element of the dataset\n",
        "print(dataset[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Size: 707\n"
          ]
        }
      ],
      "source": [
        "# Create an instance of CSVDataset\n",
        "dataset = CSVDataset(csv_file=\"./data/VBDP/train.csv\", train=False)\n",
        "# show the size of the dataset\n",
        "print(f\"Dataset Size: {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch Size: 3\n",
            "---------------\n",
            "Inputs: tensor([[1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
            "         1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
            "         0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
            "         1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 1., 1., 0., 0.]])\n",
            "Targets: tensor([2, 5, 7])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Set batch size for DataLoader\n",
        "batch_size = 3\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Iterate over the data in the DataLoader\n",
        "for batch in dataloader:\n",
        "    inputs, targets = batch\n",
        "    print(f\"Batch Size: {inputs.size(0)}\")\n",
        "    print(\"---------------\")\n",
        "    print(f\"Inputs: {inputs}\")\n",
        "    print(f\"Targets: {targets}\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import OrdinalEncoder\n",
        "# train_df = pd.read_csv('./data/VBDP/train.csv')\n",
        "# # remove the id column\n",
        "# train_df = train_df.drop(columns=['id'])\n",
        "# n_samples = train_df.shape[0]\n",
        "# n_features = train_df.shape[1] - 1\n",
        "# target_column = \"prognosis\"\n",
        "# # # Encoder our prognosis labels as integers for easier decoding later\n",
        "# enc = OrdinalEncoder()\n",
        "# train_df[target_column] = enc.fit_transform(train_df[[target_column]])\n",
        "# train_df.head()\n",
        "\n",
        "# # convert all entries to int for faster processing\n",
        "# train_df = train_df.astype(int)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Add logical combinations (AND, OR, XOR) of the features to the data set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from spotPython.utils.convert import add_logical_columns\n",
        "# df_new = train_df.copy()\n",
        "# # save the target column using \"target_column\" as the column name\n",
        "# target = train_df[target_column]\n",
        "# # remove the target column\n",
        "# df_new = df_new.drop(columns=[target_column])\n",
        "# train_df = add_logical_columns(df_new)\n",
        "# # add the target column back\n",
        "# train_df[target_column] = target\n",
        "# train_df = train_df.astype(int)\n",
        "# train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# import numpy as np\n",
        "\n",
        "# n_samples = train_df.shape[0]\n",
        "# n_features = train_df.shape[1] - 1\n",
        "# train_df.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "# train_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check content of the target column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train, X_test, y_train, y_test = train_test_split(train_df.drop(target_column, axis=1), train_df[target_column],\n",
        "#                                                     random_state=42,\n",
        "#                                                     test_size=0.25,\n",
        "#                                                     stratify=train_df[target_column])\n",
        "# trainset = pd.DataFrame(np.hstack((X_train, np.array(y_train).reshape(-1, 1))))\n",
        "# testset = pd.DataFrame(np.hstack((X_test, np.array(y_test).reshape(-1, 1))))\n",
        "# trainset.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "# testset.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "# print(train_df.shape)\n",
        "# print(trainset.shape)\n",
        "# print(testset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from spotPython.torch.dataframedataset import DataFrameDataset\n",
        "# dtype_x = torch.float32\n",
        "# dtype_y = torch.long\n",
        "# train_df = DataFrameDataset(train_df, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\n",
        "# train = DataFrameDataset(trainset, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\n",
        "# test = DataFrameDataset(testset, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\n",
        "# n_samples = len(train)\n",
        "# # add the dataset to the fun_control\n",
        "# fun_control.update({\"data\": train_df, # full dataset,\n",
        "#                \"train\": trainset,\n",
        "#                \"test\": testset,\n",
        "#                \"n_samples\": n_samples,\n",
        "#                \"target_column\": target_column})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Specification of the Preprocessing Model {#sec-specification-of-preprocessing-model-25}\n",
        "\n",
        "After the training and test data are specified and added to the `fun_control` dictionary, `spotPython` allows the specification of a data preprocessing pipeline, e.g., for the scaling of the data or for the one-hot encoding of categorical variables, see @sec-specification-of-preprocessing-model-14. This feature is not used here, so we do not change the default value (which is `None`).\n",
        "\n",
        "## Step 5: Select `algorithm` and `core_model_hyper_dict` {#sec-selection-of-the-algorithm-25}\n",
        "\n",
        "### Implementing a Configurable Neural Network With spotPython \n",
        "\n",
        "`spotPython` includes the `Net_vbdp` class which is implemented in the file `netvbdp.py`.\n",
        "The class is imported here.\n",
        "\n",
        "This class  inherits from the class `Net_Core` which is implemented in the file `netcore.py`, see @sec-the-netcore-class-14.\n",
        "\n",
        "### Add the NN Model to the fun_control Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.csvmodel import CSVModel \n",
        "from spotPython.data.light_hyper_dict import LightHyperDict\n",
        "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
        "fun_control = add_core_model_to_fun_control(core_model=CSVModel,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict= LightHyperDict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "spotPython.light.csvmodel.CSVModel"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fun_control[\"core_model\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The corresponding entries for the `core_model` class are shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'l1': {'type': 'int',\n",
              "  'default': 3,\n",
              "  'transform': 'transform_power_2_int',\n",
              "  'lower': 3,\n",
              "  'upper': 8},\n",
              " 'epochs': {'type': 'int',\n",
              "  'default': 4,\n",
              "  'transform': 'transform_power_2_int',\n",
              "  'lower': 4,\n",
              "  'upper': 9},\n",
              " 'batch_size': {'type': 'int',\n",
              "  'default': 4,\n",
              "  'transform': 'transform_power_2_int',\n",
              "  'lower': 1,\n",
              "  'upper': 4},\n",
              " 'act_fn': {'levels': ['ReLU'],\n",
              "  'type': 'factor',\n",
              "  'default': 'ReLU',\n",
              "  'transform': 'None',\n",
              "  'class_name': 'torch.nn',\n",
              "  'core_model_parameter_type': 'instance()',\n",
              "  'lower': 0,\n",
              "  'upper': 0},\n",
              " 'optimizer': {'levels': ['Adadelta',\n",
              "   'Adagrad',\n",
              "   'Adam',\n",
              "   'AdamW',\n",
              "   'SparseAdam',\n",
              "   'Adamax',\n",
              "   'ASGD',\n",
              "   'NAdam',\n",
              "   'RAdam',\n",
              "   'RMSprop',\n",
              "   'Rprop',\n",
              "   'SGD'],\n",
              "  'type': 'factor',\n",
              "  'default': 'SGD',\n",
              "  'transform': 'None',\n",
              "  'class_name': 'torch.optim',\n",
              "  'core_model_parameter_type': 'str',\n",
              "  'lower': 0,\n",
              "  'upper': 12},\n",
              " 'dropout_prob': {'type': 'float',\n",
              "  'default': 0.01,\n",
              "  'transform': 'None',\n",
              "  'lower': 0.0,\n",
              "  'upper': 0.1}}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fun_control['core_model_hyper_dict']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Modify `hyper_dict` Hyperparameters for the Selected Algorithm aka `core_model` {#sec-modification-of-hyperparameters-25}\n",
        "\n",
        " `spotPython` provides functions for modifying the hyperparameters, their bounds and factors as well as for activating and de-activating hyperparameters without re-compilation of the Python source code. These functions were described in @sec-modification-of-hyperparameters-14.\n",
        "\n",
        "::: {.callout-caution}\n",
        "### Caution: Small number of epochs for demonstration purposes\n",
        "\n",
        "* `epochs` and `patience` are set to small values for demonstration purposes. These values are too small for a real application.\n",
        "* More resonable values are, e.g.:\n",
        "  * `fun_control = modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[7, 9])` and\n",
        "  * `fun_control = modify_hyper_parameter_bounds(fun_control, \"patience\", bounds=[2, 7])`\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.hyperparameters.values import modify_hyper_parameter_bounds\n",
        "\n",
        "fun_control = modify_hyper_parameter_bounds(fun_control, \"l1\", bounds=[2,3])\n",
        "fun_control = modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[2,3])\n",
        "fun_control = modify_hyper_parameter_bounds(fun_control, \"batch_size\", bounds=[6, 8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.hyperparameters.values import modify_hyper_parameter_levels\n",
        "fun_control = modify_hyper_parameter_levels(fun_control, \"optimizer\",[\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n",
        "# fun_control = modify_hyper_parameter_levels(fun_control, \"optimizer\", [\"Adam\"])\n",
        "# fun_control[\"core_model_hyper_dict\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Selection of the Objective (Loss) Function\n",
        "\n",
        "### Evaluation  {#sec-selection-of-target-function-25}\n",
        "\n",
        "The evaluation procedure requires the specification of two elements:\n",
        "\n",
        "1. the way how the data is split into a train and a test set (see @sec-data-splitting-14)\n",
        "2. the loss function (and a metric).\n",
        "\n",
        "\n",
        "### Loss Functions and Metrics {#sec-loss-functions-and-metrics-25}\n",
        "\n",
        "The loss function is specified by the key `\"loss_function\"`.\n",
        "We will use CrossEntropy loss for the multiclass-classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from torch.nn import CrossEntropyLoss\n",
        "# loss_function = CrossEntropyLoss()\n",
        "# fun_control.update({\"loss_function\": loss_function})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Metric {#sec-metric-25}\n",
        "\n",
        "* We will use the MAP@k metric for the evaluation of the model. Here is an example how this metric is calculated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.6250)\n"
          ]
        }
      ],
      "source": [
        "from spotPython.torch.mapk import MAPK\n",
        "import torch\n",
        "mapk = MAPK(k=2)\n",
        "target = torch.tensor([0, 1, 2, 2])\n",
        "preds = torch.tensor(\n",
        "    [\n",
        "        [0.5, 0.2, 0.2],  # 0 is in top 2\n",
        "        [0.3, 0.4, 0.2],  # 1 is in top 2\n",
        "        [0.2, 0.4, 0.3],  # 2 is in top 2\n",
        "        [0.7, 0.2, 0.1],  # 2 isn't in top 2\n",
        "    ]\n",
        ")\n",
        "mapk.update(preds, target)\n",
        "print(mapk.compute()) # tensor(0.6250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from spotPython.torch.mapk import MAPK\n",
        "# import torchmetrics\n",
        "# metric_torch = MAPK(k=3)\n",
        "# fun_control.update({\"metric_torch\": metric_torch})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Calling the SPOT Function\n",
        "\n",
        "### Preparing the SPOT Call {#sec-prepare-spot-call-25}\n",
        "\n",
        "The following code passes the information about the parameter ranges and bounds to `spot`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract the variable types, names, and bounds\n",
        "from spotPython.hyperparameters.values import (get_bound_values,\n",
        "    get_var_name,\n",
        "    get_var_type,)\n",
        "var_type = get_var_type(fun_control)\n",
        "var_name = get_var_name(fun_control)\n",
        "fun_control.update({\"var_type\": var_type,\n",
        "                    \"var_name\": var_name})\n",
        "lower = get_bound_values(fun_control, \"lower\")\n",
        "upper = get_bound_values(fun_control, \"upper\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, the dictionary `fun_control` contains all information needed for the hyperparameter tuning. Before the hyperparameter tuning is started, it is recommended to take a look at the experimental design. The method `gen_design_table` generates a design table as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "fig-label": "tbl-design-25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| name         | type   | default   |   lower |   upper | transform             |\n",
            "|--------------|--------|-----------|---------|---------|-----------------------|\n",
            "| l1           | int    | 3         |       2 |     3   | transform_power_2_int |\n",
            "| epochs       | int    | 4         |       2 |     3   | transform_power_2_int |\n",
            "| batch_size   | int    | 4         |       6 |     8   | transform_power_2_int |\n",
            "| act_fn       | factor | ReLU      |       0 |     0   | None                  |\n",
            "| optimizer    | factor | SGD       |       0 |     3   | None                  |\n",
            "| dropout_prob | float  | 0.01      |       0 |     0.1 | None                  |\n"
          ]
        }
      ],
      "source": [
        "#| fig-cap: Experimental design for the hyperparameter tuning.\n",
        "from spotPython.utils.eda import gen_design_table\n",
        "print(gen_design_table(fun_control))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This allows to check if all information is available and if the information is correct.\n",
        "\n",
        "### The Objective Function `fun_torch` {#sec-the-objective-function-25}\n",
        "\n",
        "The objective function `fun_torch` is selected next. It implements an interface from `PyTorch`'s training, validation, and  testing methods to `spotPython`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.hyperlight import HyperLight\n",
        "fun = HyperLight().fun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[3.0e+00, 4.0e+00, 4.0e+00, 0.0e+00, 1.1e+01, 1.0e-02]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from spotPython.hyperparameters.values import get_default_hyperparameters_as_array\n",
        "hyper_dict=LightHyperDict().load()\n",
        "X_start = get_default_hyperparameters_as_array(fun_control, hyper_dict)\n",
        "X_start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'./runs/30-light_p040025_1min_5init_2023-06-25_20-53-30'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fun_control[\"tensorboard_path\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Starting the Hyperparameter Tuning {#sec-call-the-hyperparameter-tuner-25}\n",
        "\n",
        "The `spotPython` hyperparameter tuning is started by calling the `Spot` function as described in @sec-call-the-hyperparameter-tuner-14.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name       | Type       | Params\n",
            "------------------------------------------\n",
            "0 | act_fn     | ReLU       | 0     \n",
            "1 | train_mapk | MAPK       | 0     \n",
            "2 | valid_mapk | MAPK       | 0     \n",
            "3 | test_mapk  | MAPK       | 0     \n",
            "4 | model      | Sequential | 619   \n",
            "------------------------------------------\n",
            "619       Trainable params\n",
            "0         Non-trainable params\n",
            "619       Total params\n",
            "0.002     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "config: {'l1': 8, 'epochs': 4, 'batch_size': 256, 'act_fn': ReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.036481881978299394}\n",
            "model: CSVModel(\n",
            "  (act_fn): ReLU()\n",
            "  (train_mapk): MAPK()\n",
            "  (valid_mapk): MAPK()\n",
            "  (test_mapk): MAPK()\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=8, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.036481881978299394, inplace=False)\n",
            "    (3): Linear(in_features=8, out_features=4, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.036481881978299394, inplace=False)\n",
            "    (6): Linear(in_features=4, out_features=4, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.036481881978299394, inplace=False)\n",
            "    (9): Linear(in_features=4, out_features=2, bias=True)\n",
            "    (10): ReLU()\n",
            "    (11): Dropout(p=0.036481881978299394, inplace=False)\n",
            "    (12): Linear(in_features=2, out_features=11, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24e296441ae948429e0cfd02c3c14142",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/torch/functional.py:791: UserWarning: The operator 'aten::_unique2' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
            "  output, inverse_indices, counts = torch._unique2(\n",
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fa4da7755ff4276bc3c55fe5ec56bba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3155250bd3ca47759de26f261a1ba690",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f5610d5bf874e7586c66ab336c45b04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4316e130633f40ea90cbcec9069f20d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "705058afaba1422c83b2c81b352f3ce8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15a97d226f4040cda0a8f84f5a4ffc1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.08480565249919891    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.3962504863739014     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        valid_mapk         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.18746381998062134    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m         val_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.08480565249919891   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.3962504863739014    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       valid_mapk        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.18746381998062134   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name       | Type       | Params\n",
            "------------------------------------------\n",
            "0 | act_fn     | ReLU       | 0     \n",
            "1 | train_mapk | MAPK       | 0     \n",
            "2 | valid_mapk | MAPK       | 0     \n",
            "3 | test_mapk  | MAPK       | 0     \n",
            "4 | model      | Sequential | 301   \n",
            "------------------------------------------\n",
            "301       Trainable params\n",
            "0         Non-trainable params\n",
            "301       Total params\n",
            "0.001     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_model result: {'valid_mapk': 0.18746381998062134, 'val_loss': 2.3962504863739014, 'val_acc': 0.08480565249919891}\n",
            "\n",
            "config: {'l1': 4, 'epochs': 4, 'batch_size': 128, 'act_fn': ReLU(), 'optimizer': 'Adam', 'dropout_prob': 0.06220214613777628}\n",
            "model: CSVModel(\n",
            "  (act_fn): ReLU()\n",
            "  (train_mapk): MAPK()\n",
            "  (valid_mapk): MAPK()\n",
            "  (test_mapk): MAPK()\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=4, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.06220214613777628, inplace=False)\n",
            "    (3): Linear(in_features=4, out_features=2, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.06220214613777628, inplace=False)\n",
            "    (6): Linear(in_features=2, out_features=2, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.06220214613777628, inplace=False)\n",
            "    (9): Linear(in_features=2, out_features=1, bias=True)\n",
            "    (10): ReLU()\n",
            "    (11): Dropout(p=0.06220214613777628, inplace=False)\n",
            "    (12): Linear(in_features=1, out_features=11, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b78a9ac807b447878b20c8693c19e0d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52397bc02e8c4f9a9a15ad0624c0ac2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9c12cee9441458f972d36f3b607bb73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "184e979b82b2441884ece7ffb744c087",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa9c7f1d8dce422fbe7705680555bfd4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7efd8ce7b6ec4d429f0a9da3f9c9ff41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16c426170513406aa3c9a5631885f159",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10247349739074707    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     2.403378486633301     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        valid_mapk         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.15573559701442719    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m         val_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10247349739074707   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.403378486633301    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       valid_mapk        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.15573559701442719   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name       | Type       | Params\n",
            "------------------------------------------\n",
            "0 | act_fn     | ReLU       | 0     \n",
            "1 | train_mapk | MAPK       | 0     \n",
            "2 | valid_mapk | MAPK       | 0     \n",
            "3 | test_mapk  | MAPK       | 0     \n",
            "4 | model      | Sequential | 619   \n",
            "------------------------------------------\n",
            "619       Trainable params\n",
            "0         Non-trainable params\n",
            "619       Total params\n",
            "0.002     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_model result: {'valid_mapk': 0.15573559701442719, 'val_loss': 2.403378486633301, 'val_acc': 0.10247349739074707}\n",
            "\n",
            "config: {'l1': 8, 'epochs': 8, 'batch_size': 64, 'act_fn': ReLU(), 'optimizer': 'Adamax', 'dropout_prob': 0.0851706589553058}\n",
            "model: CSVModel(\n",
            "  (act_fn): ReLU()\n",
            "  (train_mapk): MAPK()\n",
            "  (valid_mapk): MAPK()\n",
            "  (test_mapk): MAPK()\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=8, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.0851706589553058, inplace=False)\n",
            "    (3): Linear(in_features=8, out_features=4, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.0851706589553058, inplace=False)\n",
            "    (6): Linear(in_features=4, out_features=4, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.0851706589553058, inplace=False)\n",
            "    (9): Linear(in_features=4, out_features=2, bias=True)\n",
            "    (10): ReLU()\n",
            "    (11): Dropout(p=0.0851706589553058, inplace=False)\n",
            "    (12): Linear(in_features=2, out_features=11, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "465a44600b17402da2cde6e252505945",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36140408af24404284c1328fa3ca172d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45e1c05745f5480baa79053544b4b3da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "423ede0dcd314508b3ad2f15b2d3e445",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55b2ba0aa43e43e0be5de6b39044b98a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f31be39469a49e89fbc080f63e5b1a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b2dc2d55f65439497b81db881eb9bb3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff60eee757e24ba0b71c90eaffece12a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23f8ec28a5c14110b66354e00f027fdf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cff9f1b8cf04aae8dc9de3618e2e4bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=8` reached.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e2fce6276b64d98bc2d15ca4ce8a065",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.14134275913238525    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     2.394498348236084     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        valid_mapk         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.20883488655090332    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m         val_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.14134275913238525   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.394498348236084    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       valid_mapk        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.20883488655090332   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name       | Type       | Params\n",
            "------------------------------------------\n",
            "0 | act_fn     | ReLU       | 0     \n",
            "1 | train_mapk | MAPK       | 0     \n",
            "2 | valid_mapk | MAPK       | 0     \n",
            "3 | test_mapk  | MAPK       | 0     \n",
            "4 | model      | Sequential | 301   \n",
            "------------------------------------------\n",
            "301       Trainable params\n",
            "0         Non-trainable params\n",
            "301       Total params\n",
            "0.001     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_model result: {'valid_mapk': 0.20883488655090332, 'val_loss': 2.394498348236084, 'val_acc': 0.14134275913238525}\n",
            "\n",
            "config: {'l1': 4, 'epochs': 4, 'batch_size': 128, 'act_fn': ReLU(), 'optimizer': 'AdamW', 'dropout_prob': 0.04963669926294571}\n",
            "model: CSVModel(\n",
            "  (act_fn): ReLU()\n",
            "  (train_mapk): MAPK()\n",
            "  (valid_mapk): MAPK()\n",
            "  (test_mapk): MAPK()\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=4, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.04963669926294571, inplace=False)\n",
            "    (3): Linear(in_features=4, out_features=2, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.04963669926294571, inplace=False)\n",
            "    (6): Linear(in_features=2, out_features=2, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.04963669926294571, inplace=False)\n",
            "    (9): Linear(in_features=2, out_features=1, bias=True)\n",
            "    (10): ReLU()\n",
            "    (11): Dropout(p=0.04963669926294571, inplace=False)\n",
            "    (12): Linear(in_features=1, out_features=11, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c70eb234b77e4ede92f8bb22bb8c68df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57f2bd3ecd284bcf8203a48214f51eed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "feae4df19dd2428a94fe893f7af70a09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "527ede23fc3d486e924f7e9e5c01c2ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9b3d8d92769410fa97854cb76554002",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e53658f824a406282283a541ab90482",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89828c4b9cd9484fbef4cbb2c85c5126",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.06713780760765076    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.4025814533233643     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        valid_mapk         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.16456083953380585    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m         val_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.06713780760765076   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.4025814533233643    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       valid_mapk        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.16456083953380585   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (mps), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "\n",
            "  | Name       | Type       | Params\n",
            "------------------------------------------\n",
            "0 | act_fn     | ReLU       | 0     \n",
            "1 | train_mapk | MAPK       | 0     \n",
            "2 | valid_mapk | MAPK       | 0     \n",
            "3 | test_mapk  | MAPK       | 0     \n",
            "4 | model      | Sequential | 619   \n",
            "------------------------------------------\n",
            "619       Trainable params\n",
            "0         Non-trainable params\n",
            "619       Total params\n",
            "0.002     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_model result: {'valid_mapk': 0.16456083953380585, 'val_loss': 2.4025814533233643, 'val_acc': 0.06713780760765076}\n",
            "\n",
            "config: {'l1': 8, 'epochs': 8, 'batch_size': 128, 'act_fn': ReLU(), 'optimizer': 'NAdam', 'dropout_prob': 0.016313240251430407}\n",
            "model: CSVModel(\n",
            "  (act_fn): ReLU()\n",
            "  (train_mapk): MAPK()\n",
            "  (valid_mapk): MAPK()\n",
            "  (test_mapk): MAPK()\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=64, out_features=8, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.016313240251430407, inplace=False)\n",
            "    (3): Linear(in_features=8, out_features=4, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.016313240251430407, inplace=False)\n",
            "    (6): Linear(in_features=4, out_features=4, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Dropout(p=0.016313240251430407, inplace=False)\n",
            "    (9): Linear(in_features=4, out_features=2, bias=True)\n",
            "    (10): ReLU()\n",
            "    (11): Dropout(p=0.016313240251430407, inplace=False)\n",
            "    (12): Linear(in_features=2, out_features=11, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a591651ca07d4b31b3c3219a5f30db28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e9d18ebacfa4c2b9fa17891b8789bea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "970686ae7c2b472db5b9246ac7cb71db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "980303ff552b47c5912d2124e6ddae32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba3b5e1dfd314c899e462aac49dc02e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32e19292e6a94274bb98fb685284acc5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e898081251db472c860502624ef9cf00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fd0c408553e4492bc0a5e240f93e4f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "637f4af756014d0388e6c828cfd6b7ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from spotPython.spot import spot\n",
        "from math import inf\n",
        "spot_tuner = spot.Spot(fun=fun,\n",
        "                   lower = lower,\n",
        "                   upper = upper,\n",
        "                   fun_evals = inf,\n",
        "                   fun_repeats = 1,\n",
        "                   max_time = MAX_TIME,\n",
        "                   noise = False,\n",
        "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
        "                   var_type = var_type,\n",
        "                   var_name = var_name,\n",
        "                   infill_criterion = \"y\",\n",
        "                   n_points = 1,\n",
        "                   seed=123,\n",
        "                   log_level = 50,\n",
        "                   show_models= False,\n",
        "                   show_progress= True,\n",
        "                   fun_control = fun_control,\n",
        "                   design_control={\"init_size\": INIT_SIZE,\n",
        "                                   \"repeats\": 1},\n",
        "                   surrogate_control={\"noise\": True,\n",
        "                                      \"cod_type\": \"norm\",\n",
        "                                      \"min_theta\": -4,\n",
        "                                      \"max_theta\": 3,\n",
        "                                      \"n_theta\": len(var_name),\n",
        "                                      \"model_fun_evals\": 10_000,\n",
        "                                      \"log_level\": 50\n",
        "                                      })\n",
        "spot_tuner.run(X_start=X_start)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Tensorboard {#sec-tensorboard-25}\n",
        "\n",
        "The textual output shown in the console (or code cell) can be visualized with Tensorboard as described in @sec-tensorboard-14, see also the description in the documentation: [Tensorboard.](https://sequential-parameter-optimization.github.io/spotPython/14_spot_ray_hpt_torch_cifar10.html#sec-tensorboard-14)\n",
        "\n",
        "## Step 10: Results {#sec-results-25}\n",
        "\n",
        "After the hyperparameter tuning run is finished, the results can be analyzed as described in @sec-results-14."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "fig-label": "fig-progress-25"
      },
      "outputs": [],
      "source": [
        "#| fig-cap: Progress plot. *Black* dots denote results from the initial design. *Red* dots  illustrate the improvement found by the surrogate model based optimization.\n",
        "spot_tuner.plot_progress(log_y=False, \n",
        "    filename=\"./figures/\" + experiment_name+\"_progress.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "fig-label": "tbl-results-25"
      },
      "outputs": [],
      "source": [
        "#| fig-cap: Results of the hyperparameter tuning.\n",
        "from spotPython.utils.eda import gen_design_table\n",
        "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "fig-label": "fig-importance-25"
      },
      "outputs": [],
      "source": [
        "#| fig-cap: 'Variable importance plot, threshold 0.025.'\n",
        "spot_tuner.plot_importance(threshold=0.025,\n",
        "    filename=\"./figures/\" + experiment_name+\"_importance.png\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get the Data Without Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "train_df = pd.read_csv('./data/VBDP/train.csv')\n",
        "# remove the id column\n",
        "train_df = train_df.drop(columns=['id'])\n",
        "n_samples = train_df.shape[0]\n",
        "n_features = train_df.shape[1] - 1\n",
        "target_column = \"prognosis\"\n",
        "# # Encoder our prognosis labels as integers for easier decoding later\n",
        "enc = OrdinalEncoder()\n",
        "train_df[target_column] = enc.fit_transform(train_df[[target_column]])\n",
        "train_df.head()\n",
        "\n",
        "# convert all entries to int for faster processing\n",
        "train_df = train_df.astype(int)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "n_samples = train_df.shape[0]\n",
        "n_features = train_df.shape[1] - 1\n",
        "train_df.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "train_df.head()\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df.drop(target_column, axis=1), train_df[target_column],\n",
        "                                                    random_state=42,\n",
        "                                                    test_size=0.25,\n",
        "                                                    stratify=train_df[target_column])\n",
        "trainset = pd.DataFrame(np.hstack((X_train, np.array(y_train).reshape(-1, 1))))\n",
        "testset = pd.DataFrame(np.hstack((X_test, np.array(y_test).reshape(-1, 1))))\n",
        "trainset.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "testset.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "print(train_df.shape)\n",
        "print(trainset.shape)\n",
        "print(testset.shape)\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from spotPython.torch.dataframedataset import DataFrameDataset\n",
        "dtype_x = torch.float32\n",
        "dtype_y = torch.long\n",
        "train_df = DataFrameDataset(train_df, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\n",
        "train = DataFrameDataset(trainset, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\n",
        "test = DataFrameDataset(testset, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\n",
        "n_samples = len(train)\n",
        "# add the dataset to the fun_control\n",
        "fun_control.update({\"data\": train_df, # full dataset,\n",
        "               \"train\": trainset,\n",
        "               \"test\": testset,\n",
        "               \"n_samples\": n_samples,\n",
        "               \"target_column\": target_column})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get the Tuned Architecture {#sec-get-spot-results-25}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.hyperparameters.values import get_one_config_from_X\n",
        "X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\n",
        "config = get_one_config_from_X(X, fun_control)\n",
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.traintest import test_model\n",
        "test_model(config, fun_control)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cross Validation With Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.traintest import cv_model\n",
        "cv_model(config, fun_control)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.light.kfolddatamodule import KFoldDataModule\n",
        "import lightning as L\n",
        "results = []\n",
        "nums_folds = 10\n",
        "split_seed = 12345\n",
        "\n",
        "for k in range(nums_folds):\n",
        "    dm = KFoldDataModule(k=k, num_folds=num_folds, split_seed=split_seed)\n",
        "    dm.prepare_data()\n",
        "    dm.setup()\n",
        "\n",
        "    # here we train the model on given split...\n",
        "    print(f\"model: {model}\")\n",
        "    # Init trainer\n",
        "    trainer = L.Trainer(\n",
        "        max_epochs=model.epochs,\n",
        "        accelerator=\"auto\",\n",
        "        devices=1,\n",
        "        # logger=TensorBoardLogger(save_dir=fun_control[\"tensorboard_path\"], version=config_id),\n",
        "    )\n",
        "    # Pass the datamodule as arg to trainer.fit to override model hooks :)\n",
        "    trainer.fit(model=model, datamodule=dm)\n",
        "    # Test best model on validation and test set\n",
        "    # result = trainer.validate(model=model, datamodule=dm, ckpt_path=\"last\")\n",
        "    score = trainer.validate(model=model, datamodule=dm)\n",
        "    # unlist the result (from a list of one dict)\n",
        "    score = score[0]\n",
        "    print(f\"train_model result: {score}\")\n",
        "\n",
        "    results.append(score)\n",
        "\n",
        "score = sum(results) / num_folds"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation of the Tuned Architecture Without Lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "train_df = pd.read_csv('./data/VBDP/train.csv')\n",
        "# remove the id column\n",
        "train_df = train_df.drop(columns=['id'])\n",
        "n_samples = train_df.shape[0]\n",
        "n_features = train_df.shape[1] - 1\n",
        "target_column = \"prognosis\"\n",
        "# # Encoder our prognosis labels as integers for easier decoding later\n",
        "enc = OrdinalEncoder()\n",
        "train_df[target_column] = enc.fit_transform(train_df[[target_column]])\n",
        "train_df.head()\n",
        "\n",
        "# convert all entries to int for faster processing\n",
        "train_df = train_df.astype(int)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "n_samples = train_df.shape[0]\n",
        "n_features = train_df.shape[1] - 1\n",
        "train_df.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "train_df.head()\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df.drop(target_column, axis=1), train_df[target_column],\n",
        "                                                    random_state=42,\n",
        "                                                    test_size=0.25,\n",
        "                                                    stratify=train_df[target_column])\n",
        "trainset = pd.DataFrame(np.hstack((X_train, np.array(y_train).reshape(-1, 1))))\n",
        "testset = pd.DataFrame(np.hstack((X_test, np.array(y_test).reshape(-1, 1))))\n",
        "trainset.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "testset.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "print(train_df.shape)\n",
        "print(trainset.shape)\n",
        "print(testset.shape)\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from spotPython.torch.dataframedataset import DataFrameDataset\n",
        "dtype_x = torch.float32\n",
        "dtype_y = torch.long\n",
        "train_df = DataFrameDataset(train_df, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\n",
        "train = DataFrameDataset(trainset, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\n",
        "test = DataFrameDataset(testset, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\n",
        "n_samples = len(train)\n",
        "# add the dataset to the fun_control\n",
        "fun_control.update({\"data\": train_df, # full dataset,\n",
        "               \"train\": trainset,\n",
        "               \"test\": testset,\n",
        "               \"n_samples\": n_samples,\n",
        "               \"target_column\": target_column})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.hyperparameters.values import get_one_core_model_from_X\n",
        "X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\n",
        "model_spot = get_one_core_model_from_X(X, fun_control)\n",
        "model_spot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.torch.traintest import (\n",
        "    train_tuned,\n",
        "    test_tuned,\n",
        "    )\n",
        "train_tuned(net=model_spot, train_dataset=train,\n",
        "        loss_function=fun_control[\"loss_function\"],\n",
        "        metric=fun_control[\"metric_torch\"],\n",
        "        shuffle=True,\n",
        "        device = fun_control[\"device\"],\n",
        "        path=None,\n",
        "        task=fun_control[\"task\"],)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If `path` is set to a filename, e.g., `path = \"model_spot_trained.pt\"`, the weights of the trained model will be loaded from this file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_tuned(net=model_spot, test_dataset=test,\n",
        "            shuffle=False,\n",
        "            loss_function=fun_control[\"loss_function\"],\n",
        "            metric=fun_control[\"metric_torch\"],\n",
        "            device = fun_control[\"device\"],\n",
        "            task=fun_control[\"task\"],)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-validated Evaluations\n",
        "\n",
        "* This is the evaluation that will be used in the comparison.\n",
        "\n",
        "::: {.callout-caution}\n",
        "### Caution: Cross-validated Evaluations\n",
        "\n",
        "* The number of folds is set to 1 by default.\n",
        "* Here it was changed to 3 for demonstration purposes.\n",
        "* Set the number of folds to a reasonable value, e.g., 10.\n",
        "* This can be done by setting the `k_folds` attribute of the model as follows:\n",
        "* `setattr(model_spot, \"k_folds\",  10)`\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.torch.traintest import evaluate_cv\n",
        "# modify k-kolds:\n",
        "setattr(model_spot, \"k_folds\",  3)\n",
        "df_eval, df_preds, df_metrics = evaluate_cv(net=model_spot,\n",
        "    dataset=fun_control[\"data\"],\n",
        "    loss_function=fun_control[\"loss_function\"],\n",
        "    metric=fun_control[\"metric_torch\"],\n",
        "    task=fun_control[\"task\"],\n",
        "    writer=fun_control[\"writer\"],\n",
        "    writerId=\"model_spot_cv\",\n",
        "    device = fun_control[\"device\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metric_name = type(fun_control[\"metric_torch\"]).__name__\n",
        "print(f\"loss: {df_eval}, Cross-validated {metric_name}: {df_metrics}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Hyperparameter Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "fig-label": "fig-contour-25"
      },
      "outputs": [],
      "source": [
        "#| fig-cap: Contour plots.\n",
        "filename = \"./figures/\" + experiment_name\n",
        "spot_tuner.plot_important_hyperparameter_contour(filename=filename)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parallel Coordinates Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "fig-label": "fig-parallel-25"
      },
      "outputs": [],
      "source": [
        "#| fig-cap: Parallel coordinates plots\n",
        "spot_tuner.parallel_plot()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot all Combinations of Hyperparameters\n",
        "\n",
        "* Warning: this may take a while."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PLOT_ALL = False\n",
        "if PLOT_ALL:\n",
        "    n = spot_tuner.k\n",
        "    for i in range(n-1):\n",
        "        for j in range(i+1, n):\n",
        "            spot_tuner.plot_contour(i=i, j=j, min_z=min_z, max_z = max_z)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
