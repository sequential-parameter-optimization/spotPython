{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import inf\n",
        "import torch\n",
        "import torchmetrics\n",
        "from torch.nn import CrossEntropyLoss, NLLLoss\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from spotPython.spot import spot\n",
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.hyperparameters.values import (\n",
        "    add_core_model_to_fun_control,\n",
        "    modify_hyper_parameter_levels,\n",
        "    modify_hyper_parameter_bounds,\n",
        "    get_var_type,\n",
        "    get_var_name,\n",
        "    get_bound_values,\n",
        "    get_one_core_model_from_X,\n",
        "    get_default_hyperparameters_as_array\n",
        "    )\n",
        "from spotPython.data.torch_hyper_dict import TorchHyperDict\n",
        "from spotPython.fun.hypertorch import HyperTorch\n",
        "from spotPython.torch.netvbdp import Net_vbdp\n",
        "from spotPython.torch.traintest import (\n",
        "    train_tuned,\n",
        "    test_tuned,\n",
        "    )\n",
        "from spotPython.torch.dataframedataset import DataFrameDataset\n",
        "from spotPython.torch.mapk import MAPK\n",
        "from spotPython.data.vbdp import modify_vbdp_dataframe, combine_features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "fun_control = fun_control_init(task=\"classification\", tensorboard_path=\"runs/25_spot_torch_vbdp\")\n",
        "fun_control.update({\"show_batch_interval\": 100_000_000})\n",
        "# load data\n",
        "train_df = pd.read_csv('./data/VBDP/train.csv')\n",
        "# remove the id column\n",
        "train_df = train_df.drop(columns=['id'])\n",
        "n_samples = train_df.shape[0]\n",
        "n_features = train_df.shape[1] - 1\n",
        "target_column = \"prognosis\"\n",
        "# # Encoder our prognosis labels as integers for easier decoding later\n",
        "enc = OrdinalEncoder()\n",
        "train_df[target_column] = enc.fit_transform(train_df[[target_column]])\n",
        "train_df.head()\n",
        "\n",
        "# convert all entries to int for faster processing\n",
        "train_df = train_df.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/dw/pvtj6mt91znd0hftcztqb0k00000gn/T/ipykernel_12542/3336454340.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[target_column] = target\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sudden_fever</th>\n",
              "      <th>headache</th>\n",
              "      <th>mouth_bleed</th>\n",
              "      <th>nose_bleed</th>\n",
              "      <th>muscle_pain</th>\n",
              "      <th>joint_pain</th>\n",
              "      <th>vomiting</th>\n",
              "      <th>rash</th>\n",
              "      <th>diarrhea</th>\n",
              "      <th>hypotension</th>\n",
              "      <th>...</th>\n",
              "      <th>6039</th>\n",
              "      <th>6040</th>\n",
              "      <th>6041</th>\n",
              "      <th>6042</th>\n",
              "      <th>6043</th>\n",
              "      <th>6044</th>\n",
              "      <th>6045</th>\n",
              "      <th>6046</th>\n",
              "      <th>6047</th>\n",
              "      <th>prognosis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 6113 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   sudden_fever  headache  mouth_bleed  nose_bleed  muscle_pain  joint_pain   \n",
              "0             1         1            0           1            1           1  \\\n",
              "1             0         0            0           0            0           0   \n",
              "2             0         1            1           1            0           1   \n",
              "3             0         0            1           1            1           1   \n",
              "4             0         0            0           0            0           0   \n",
              "\n",
              "   vomiting  rash  diarrhea  hypotension  ...  6039  6040  6041  6042  6043   \n",
              "0         1     0         1            1  ...     0     0     0     0     0  \\\n",
              "1         1     0         1            0  ...     0     0     0     0     0   \n",
              "2         1     1         1            1  ...     1     1     0     1     1   \n",
              "3         0     1         0            1  ...     0     0     0     0     0   \n",
              "4         0     0         1            0  ...     0     1     1     0     1   \n",
              "\n",
              "   6044  6045  6046  6047  prognosis  \n",
              "0     0     0     0     0          3  \n",
              "1     0     0     0     0          7  \n",
              "2     0     1     1     0          3  \n",
              "3     0     0     0     0         10  \n",
              "4     1     0     0     0          6  \n",
              "\n",
              "[5 rows x 6113 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_new = train_df.copy()\n",
        "# save the target column using \"target_column\" as the column name\n",
        "target = train_df[target_column]\n",
        "# remove the target column\n",
        "df_new = df_new.drop(columns=[target_column])\n",
        "train_df = combine_features(df_new)\n",
        "# add the target column back\n",
        "train_df[target_column] = target\n",
        "train_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* feature engineering: 6112 features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(707, 6113)\n",
            "(530, 6113)\n",
            "(177, 6113)\n"
          ]
        }
      ],
      "source": [
        "n_samples = train_df.shape[0]\n",
        "n_features = train_df.shape[1] - 1\n",
        "train_df.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df.drop(target_column, axis=1), train_df[target_column],\n",
        "                                                    random_state=42,\n",
        "                                                    test_size=0.25,\n",
        "                                                    stratify=train_df[target_column])\n",
        "trainset = pd.DataFrame(np.hstack((X_train, np.array(y_train).reshape(-1, 1))))\n",
        "testset = pd.DataFrame(np.hstack((X_test, np.array(y_test).reshape(-1, 1))))\n",
        "trainset.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "testset.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "print(train_df.shape)\n",
        "print(trainset.shape)\n",
        "print(testset.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "dtype_x = torch.float32\n",
        "dtype_y = torch.long\n",
        "train_df = DataFrameDataset(train_df, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\n",
        "train = DataFrameDataset(trainset, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\n",
        "test = DataFrameDataset(testset, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\n",
        "n_samples = len(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add the dataset to the fun_control\n",
        "fun_control.update({\"data\": train_df, # full dataset,\n",
        "               \"train\": train,\n",
        "               \"test\": test,\n",
        "               \"n_samples\": n_samples,\n",
        "               \"target_column\": target_column})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "config: {'_L0': 6112, 'l1': 32768, 'dropout_prob': 0.7103122166156, 'lr_mult': 0.001, 'batch_size': 4, 'epochs': 64, 'k_folds': 1, 'patience': 64, 'optimizer': 'AdamW', 'sgd_momentum': 0.9}\n",
            "Epoch: 1\n",
            "Loss on hold-out set: 2.397013398836244\n",
            "Accuracy on hold-out set: 0.14622641509433962\n",
            "MAPK value on hold-out data: 0.23506289308176098\n",
            "Epoch: 2\n",
            "Loss on hold-out set: 2.3962632890017526\n",
            "Accuracy on hold-out set: 0.12735849056603774\n",
            "MAPK value on hold-out data: 0.2272012578616352\n",
            "Epoch: 3\n",
            "Loss on hold-out set: 2.3953690034038617\n",
            "Accuracy on hold-out set: 0.14622641509433962\n",
            "MAPK value on hold-out data: 0.23977987421383645\n",
            "Epoch: 4\n",
            "Loss on hold-out set: 2.394227927585818\n",
            "Accuracy on hold-out set: 0.13679245283018868\n",
            "MAPK value on hold-out data: 0.2366352201257861\n",
            "Epoch: 5\n",
            "Loss on hold-out set: 2.3928299624964877\n",
            "Accuracy on hold-out set: 0.13679245283018868\n",
            "MAPK value on hold-out data: 0.23663522012578614\n",
            "Epoch: 6\n",
            "Loss on hold-out set: 2.3910861375196926\n",
            "Accuracy on hold-out set: 0.13679245283018868\n",
            "MAPK value on hold-out data: 0.23506289308176098\n",
            "Epoch: 7\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# add the nn model to the fun_control dictionary\n",
        "fun_control = add_core_model_to_fun_control(core_model=Net_vbdp,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=TorchHyperDict)\n",
        "# modify the hyperparameter levels\n",
        "fun_control = modify_hyper_parameter_bounds(fun_control, \"_L0\", bounds=[n_features, n_features])\n",
        "# fun_control = modify_hyper_parameter_bounds(fun_control, \"l1\", bounds=[3, 4])\n",
        "# fun_control = modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[2, 9])\n",
        "fun_control = modify_hyper_parameter_bounds(fun_control, \"patience\", bounds=[2, 6])\n",
        "fun_control = modify_hyper_parameter_bounds(fun_control, \"lr_mult\", bounds=[1e-3, 1e-3])\n",
        "fun_control = modify_hyper_parameter_bounds(fun_control, \"sgd_momentum\", bounds=[0.9, 0.9])\n",
        "fun_control = modify_hyper_parameter_levels(fun_control, \"optimizer\",[\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n",
        "# select metric and loss function\n",
        "# metric_torch = torchmetrics.Accuracy(task=\"multiclass\", num_classes=11)\n",
        "metric_torch = MAPK(k=3)\n",
        "loss_torch = CrossEntropyLoss()\n",
        "# loss_torch = NLLLoss()\n",
        "fun_control.update({\n",
        "               \"metric_torch\": metric_torch,\n",
        "               \"loss_function\": loss_torch,\n",
        "               \"device\": \"cpu\",\n",
        "               })\n",
        "# extract the variable types, names, and bounds\n",
        "var_type = get_var_type(fun_control)\n",
        "var_name = get_var_name(fun_control)\n",
        "fun_control.update({\"var_type\": var_type,\n",
        "                    \"var_name\": var_name})\n",
        "lower = get_bound_values(fun_control, \"lower\")\n",
        "upper = get_bound_values(fun_control, \"upper\")\n",
        "\n",
        "# get the default hyperparameters as array\n",
        "hyper_dict=TorchHyperDict().load()\n",
        "X_start = get_default_hyperparameters_as_array(fun_control, hyper_dict)\n",
        "\n",
        "# get the objective function\n",
        "fun = HyperTorch().fun_torch\n",
        "\n",
        "# initialize spot\n",
        "spot_tuner = spot.Spot(fun=fun,\n",
        "                   lower = lower,\n",
        "                   upper = upper,\n",
        "                   fun_evals = inf,\n",
        "                   fun_repeats = 1,\n",
        "                   max_time = 1,\n",
        "                   noise = False,\n",
        "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
        "                   var_type = var_type,\n",
        "                   var_name = var_name,\n",
        "                   infill_criterion = \"y\",\n",
        "                   n_points = 1,\n",
        "                   seed=123,\n",
        "                   log_level = 50,\n",
        "                   show_models= False,\n",
        "                   show_progress= True,\n",
        "                   fun_control = fun_control,\n",
        "                   design_control={\"init_size\": 5,\n",
        "                                   \"repeats\": 1},\n",
        "                   surrogate_control={\"noise\": True,\n",
        "                                      \"cod_type\": \"norm\",\n",
        "                                      \"min_theta\": -4,\n",
        "                                      \"max_theta\": 3,\n",
        "                                      \"n_theta\": len(var_name),\n",
        "                                      \"model_fun_evals\": 10_000,\n",
        "                                      \"log_level\": 50\n",
        "                                      })\n",
        "# run spot\n",
        "spot_tuner.run(X_start=X_start)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "spot_tuner.plot_progress()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "from spotPython.utils.eda import gen_design_table\n",
        "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "spot_tuner.plot_importance()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\n",
        "model_spot = get_one_core_model_from_X(X, fun_control)\n",
        "model_spot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.torch.mapk import MAPK\n",
        "metric_torch = MAPK(k=3)\n",
        "fun_control.update({\n",
        "               \"metric_torch\": metric_torch,               \n",
        "               })\n",
        "\n",
        "train_tuned(net=model_spot, train_dataset=train,\n",
        "        loss_function=fun_control[\"loss_function\"],\n",
        "        metric=fun_control[\"metric_torch\"],\n",
        "        shuffle=True,\n",
        "        device = \"cpu\",\n",
        "        path=None,\n",
        "        task=fun_control[\"task\"],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_tuned(net=model_spot, test_dataset=test,\n",
        "            shuffle=False,\n",
        "            loss_function=fun_control[\"loss_function\"],\n",
        "            metric=fun_control[\"metric_torch\"],\n",
        "            device = \"cpu\",\n",
        "            task=fun_control[\"task\"],)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-validated Evaluations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* This is the evaluation that will be used in the comparison (evaluatecv has to be updated before, to get metric vlaues!):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.torch.traintest import evaluate_cv\n",
        "# modify k-kolds:+\n",
        "setattr(model_spot, \"k_folds\",  10)\n",
        "evaluate_cv(net=model_spot, dataset=fun_control[\"data\"], loss_function=fun_control[\"loss_function\"], metric=fun_control[\"metric_torch\"], task=fun_control[\"task\"], writer=fun_control[\"writer\"], writerId=\"model_spot_cv\", device=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "spot_tuner.plot_important_hyperparameter_contour()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# close tensorbaoard writer\n",
        "if fun_control[\"writer\"] is not None:\n",
        "    fun_control[\"writer\"].close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
