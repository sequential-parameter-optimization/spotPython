{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: PyTorch Hyperparameter Tuning --- A Tutorial for spotPython\n",
        "subtitle: Version 0.2.14\n",
        "format:\n",
        "  pdf:\n",
        "    template: bart23e_template.tex\n",
        "    fig-width: 7\n",
        "    fig-height: 5\n",
        "    keep-tex: true\n",
        "    linenumbers: false\n",
        "    doublespacing: false\n",
        "    number-sections: true\n",
        "    runninghead: PyTorch Hyperparameter Tuning With spotPython\n",
        "  html:\n",
        "    fig-width: 7\n",
        "    fig-height: 5\n",
        "author:\n",
        "  - name: Thomas Bartz-Beielstein\n",
        "    affiliations:\n",
        "      - name: SpotSeven Lab\n",
        "      - city: Gummersbach\n",
        "        country: Germany\n",
        "        postal-code: 51643\n",
        "    orcid: 0000-0002-5938-5158\n",
        "    email: bartzbeielstein@gmail.com\n",
        "    url: 'https://www.spotseven.de'\n",
        "abstract: |\n",
        "  The goal of hyperparameter tuning (or hyperparameter optimization) is to optimize the hyperparameters to improve the performance of the machine or deep learning model. spotPython (\"Sequential Parameter Optimization Toolbox in Python\") is the Python version of the well-known hyperparameter tuner SPOT, which has been developed in the R programming environment for statistical analysis for over a decade. PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. This document shows how to integrate the spotPython hyperparameter tuner into the PyTorch training workflow.  As an example, the results of the CIFAR10 image classifier are used. In addition to an introduction to spotPython, this tutorial also includes a brief comparison with Ray Tune, a Python library for running experiments and tuning hyperparameters. This comparison is based on the PyTorch hyperparameter tuning tutorial. The advantages and disadvantages of both approaches are discussed. We show that spotPython achieves similar or even better results while being more flexible and transparent than Ray Tune.\n",
        "keywords:\n",
        "  - hyperparameter tuning\n",
        "  - hyperparameter optimization\n",
        "  - spotPython\n",
        "  - PyTorch\n",
        "  - CIFAR10\n",
        "  - optimization\n",
        "  - deep learning\n",
        "  - machine learning\n",
        "bibliography: bart23e.bib\n",
        "execute:\n",
        "  cache: false\n",
        "  eval: false\n",
        "  echo: true\n",
        "  warning: false\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Tuning {#sec-hyperparameter-tuning}\n",
        "\n",
        "Hyperparameter tuning is an important, but often difficult and computationally intensive task.\n",
        "Changing the architecture of a neural network or the learning rate of an optimizer can have a significant impact on the performance.\n",
        "\n",
        "The goal of hyperparameter tuning is to optimize the hyperparameters in a way that improves the performance of the machine learning or deep learning model.\n",
        "The simplest, but also most computationally expensive, approach uses manual search (or trial-and-error [@Meignan:2015vp]).\n",
        "Commonly encountered is simple random search, i.e., random and repeated selection of hyperparameters for evaluation, and lattice search (\"grid search\").\n",
        "In addition, methods that perform directed search  and other model-free algorithms, i.e., algorithms that do not explicitly rely on a model, e.g., evolution strategies [@Bart13j] or pattern search [@Torczon00] play an important role.\n",
        "Also, \"hyperband\", i.e., a multi-armed bandit strategy that dynamically allocates resources to a set of random configurations and uses successive bisections to stop configurations with poor performance [@Li16a], is very common in hyperparameter tuning.\n",
        "The most sophisticated and efficient approaches are the Bayesian optimization and surrogate model based optimization methods, which are based on the optimization of cost functions determined by simulations or experiments.\n",
        "\n",
        "We consider below a surrogate model based optimization-based hyperparameter tuning approach based on the Python version of the SPOT (\"Sequential Parameter Optimization Toolbox\") [@BLP05], which is suitable for situations where only limited resources are available. This may be due to limited availability and cost of hardware, or due to the fact that confidential data may only be processed locally, e.g., due to legal requirements.\n",
        "Furthermore, in our approach, the understanding of algorithms is seen as a key tool for enabling transparency and explainability. This can be enabled, for example, by quantifying the contribution of machine learning and deep learning components (nodes, layers, split decisions, activation functions, etc.).\n",
        "Understanding the importance of hyperparameters and the interactions between multiple hyperparameters plays a major role in the interpretability and explainability of machine learning models.\n",
        "SPOT provides statistical tools for understanding hyperparameters and their interactions. Last but not least, it should be noted that the SPOT software code is available in the open source `spotPython` package on github^[[https://github.com/sequential-parameter-optimization](https://github.com/sequential-parameter-optimization)], allowing replicability of the results.\n",
        "This tutorial descries the Python variant of SPOT, which is called `spotPython`. The R implementation is described in @bart21i.\n",
        "SPOT is an established open source software that has been maintained for more than 15 years [@BLP05] [@bart21i].\n",
        "\n",
        "This tutorial is structured as follows. The concept of the hyperparameter tuning software `spotPython` is described in @sec-spot. \n",
        "@sec-quickstart (\"Quickstart\") describes the execution of the example from the tutorial \"Hyperparameter Tuning with Ray Tune\" [@pyto23a].\n",
        "@sec-hyperparameter-tuning-for-pytorch describes the integration of `spotPython` into the ``PyTorch`` training workflow in detail and presents the results. Finally, @sec-summary presents a summary and an outlook.\n",
        "\n",
        "::: {.callout-note}\n",
        "The corresponding ` .ipynb` notebook [@bart23e] is updated regularly and reflects updates and changes in the `spotPython` package.\n",
        "It can be downloaded from [https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb](https://github.com/sequential-parameter-optimization/spotPython/blob/main/notebooks/14_spot_ray_hpt_torch_cifar10.ipynb).\n",
        ":::\n",
        "\n",
        "\n",
        "# The Hyperparameter Tuning Software SPOT {#sec-spot}\n",
        "\n",
        "Surrogate model based optimization methods are common approaches in simulation and optimization. SPOT was developed because there is a great need for sound statistical analysis of simulation and optimization algorithms. SPOT includes methods for tuning based on classical regression and analysis of variance techniques.\n",
        "It presents tree-based models such as classification and regression trees and random forests as well as Bayesian optimization (Gaussian process models, also known as Kriging). Combinations of different meta-modeling approaches are possible. SPOT comes with a sophisticated surrogate model based optimization method, that can handle discrete and continuous inputs. Furthermore, any model implemented in `scikit-learn` can be used out-of-the-box as a surrogate in `spotPython`.\n",
        "\n",
        "SPOT implements key techniques such as exploratory fitness landscape analysis and sensitivity analysis. It can be used to understand the performance of various algorithms, while simultaneously giving insights into their algorithmic behavior.\n",
        "In addition, SPOT can be used as an optimizer and for automatic and interactive tuning. Details on SPOT and its use in practice are given by @bart21i.\n",
        "\n",
        "A typical hyperparameter tuning process with `spotPython` consists of the following steps:\n",
        "\n",
        "1. Loading the data (training and test datasets), see @sec-data-loading.\n",
        "2. Specification of the preprocessing model, see @sec-specification-of-preprocessing-model. This model is called `prep_model` (\"preparation\" or pre-processing).\n",
        "The information required for the hyperparameter tuning is stored in the dictionary `fun_control`. Thus, the information needed for the execution of the hyperparameter tuning is available in a readable form.\n",
        "3. Selection of the machine learning or deep learning model to be tuned, see @sec-selection-of-the-algorithm. This is called the `core_model`. Once the `core_model` is defined, then the associated hyperparameters are stored in the `fun_control` dictionary. First, the hyperparameters of the `core_model` are initialized with the default values of the `core_model`.\n",
        "As default values we use the default values contained in the `spotPython` package for the algorithms of the `torch` package.\n",
        "4. Modification of the default values for the hyperparameters used in `core_model`, see @sec-modification-of-default-values. This step is optional.\n",
        "   1. numeric parameters are modified by changing the bounds.\n",
        "   2. categorical parameters are modified by changing the categories (\"levels\").\n",
        "5. Selection of target function (loss function) for the optimizer, see @sec-selection-of-target-function.\n",
        "6. Calling SPOT with the corresponding parameters, see @sec-call-the-hyperparameter-tuner. The results are stored in a dictionary and are available for further analysis.\n",
        "7. Presentation, visualization and interpretation of the results, see @sec-results-tuning.\n",
        "\n",
        "\n",
        "# Quickstart {#sec-quickstart}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from math import inf\n",
        "import torch\n",
        "import torchmetrics\n",
        "from torch.nn import CrossEntropyLoss, NLLLoss\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from spotPython.spot import spot\n",
        "from spotPython.utils.init import fun_control_init\n",
        "from spotPython.data.torchdata import load_data_cifar10\n",
        "from spotPython.hyperparameters.values import (\n",
        "    add_core_model_to_fun_control,\n",
        "    modify_hyper_parameter_levels,\n",
        "    modify_hyper_parameter_bounds,\n",
        "    get_var_type,\n",
        "    get_var_name,\n",
        "    get_bound_values,\n",
        "    get_one_core_model_from_X,\n",
        "    get_default_hyperparameters_as_array\n",
        "    )\n",
        "from spotPython.data.torch_hyper_dict import TorchHyperDict\n",
        "from spotPython.fun.hypertorch import HyperTorch\n",
        "from spotPython.torch.netvbdp import Net_vbdp\n",
        "# from spotPython.torch.netcifar10 import Net_CIFAR10\n",
        "from spotPython.torch.traintest import (\n",
        "    train_tuned,\n",
        "    test_tuned,\n",
        "    )\n",
        "from spotPython.torch.dataframedataset import DataFrameDataset\n",
        "from spotPython.torch.mapk import MAPK\n",
        "from spotPython.data.vbdp import modify_vbdp_dataframe, combine_features"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#| echo: true\n",
        "#| eval: false\n",
        "fun_control = fun_control_init(task=\"classification\", tensorboard_path=\"runs/25_spot_torch_vbdp\")\n",
        "fun_control.update({\"show_batch_interval\": 100_000_000})\n",
        "# load data\n",
        "train_df = pd.read_csv('./data/VBDP/train.csv')\n",
        "# remove the id column\n",
        "train_df = train_df.drop(columns=['id'])\n",
        "n_samples = train_df.shape[0]\n",
        "n_features = train_df.shape[1] - 1\n",
        "target_column = \"prognosis\"\n",
        "# # Encoder our prognosis labels as integers for easier decoding later\n",
        "enc = OrdinalEncoder()\n",
        "train_df[target_column] = enc.fit_transform(train_df[[target_column]])\n",
        "train_df.head()\n",
        "\n",
        "# convert all entries to int for faster processing\n",
        "train_df = train_df.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/hf/rnzlxrlx0fq3fr59krvfr1800000gn/T/ipykernel_46069/3336454340.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  train_df[target_column] = target\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sudden_fever</th>\n",
              "      <th>headache</th>\n",
              "      <th>mouth_bleed</th>\n",
              "      <th>nose_bleed</th>\n",
              "      <th>muscle_pain</th>\n",
              "      <th>joint_pain</th>\n",
              "      <th>vomiting</th>\n",
              "      <th>rash</th>\n",
              "      <th>diarrhea</th>\n",
              "      <th>hypotension</th>\n",
              "      <th>...</th>\n",
              "      <th>6039</th>\n",
              "      <th>6040</th>\n",
              "      <th>6041</th>\n",
              "      <th>6042</th>\n",
              "      <th>6043</th>\n",
              "      <th>6044</th>\n",
              "      <th>6045</th>\n",
              "      <th>6046</th>\n",
              "      <th>6047</th>\n",
              "      <th>prognosis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 6113 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   sudden_fever  headache  mouth_bleed  nose_bleed  muscle_pain  joint_pain   \n",
              "0             1         1            0           1            1           1  \\\n",
              "1             0         0            0           0            0           0   \n",
              "2             0         1            1           1            0           1   \n",
              "3             0         0            1           1            1           1   \n",
              "4             0         0            0           0            0           0   \n",
              "\n",
              "   vomiting  rash  diarrhea  hypotension  ...  6039  6040  6041  6042  6043   \n",
              "0         1     0         1            1  ...     0     0     0     0     0  \\\n",
              "1         1     0         1            0  ...     0     0     0     0     0   \n",
              "2         1     1         1            1  ...     1     1     0     1     1   \n",
              "3         0     1         0            1  ...     0     0     0     0     0   \n",
              "4         0     0         1            0  ...     0     1     1     0     1   \n",
              "\n",
              "   6044  6045  6046  6047  prognosis  \n",
              "0     0     0     0     0          3  \n",
              "1     0     0     0     0          7  \n",
              "2     0     1     1     0          3  \n",
              "3     0     0     0     0         10  \n",
              "4     1     0     0     0          6  \n",
              "\n",
              "[5 rows x 6113 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_new = train_df.copy()\n",
        "# save the target column using \"target_column\" as the column name\n",
        "target = train_df[target_column]\n",
        "# remove the target column\n",
        "df_new = df_new.drop(columns=[target_column])\n",
        "train_df = combine_features(df_new)\n",
        "# add the target column back\n",
        "train_df[target_column] = target\n",
        "train_df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* feature engineering: 6112 features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(707, 6113)\n",
            "(530, 6113)\n",
            "(177, 6113)\n"
          ]
        }
      ],
      "source": [
        "n_samples = train_df.shape[0]\n",
        "n_features = train_df.shape[1] - 1\n",
        "train_df.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df.drop(target_column, axis=1), train_df[target_column],\n",
        "                                                    random_state=42,\n",
        "                                                    test_size=0.25,\n",
        "                                                    stratify=train_df[target_column])\n",
        "trainset = pd.DataFrame(np.hstack((X_train, np.array(y_train).reshape(-1, 1))))\n",
        "testset = pd.DataFrame(np.hstack((X_test, np.array(y_test).reshape(-1, 1))))\n",
        "trainset.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "testset.columns = [f\"x{i}\" for i in range(1, n_features+1)] + [target_column]\n",
        "print(train_df.shape)\n",
        "print(trainset.shape)\n",
        "print(testset.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "dtype_x = torch.float32\n",
        "dtype_y = torch.long\n",
        "train_df = DataFrameDataset(train_df, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\n",
        "train = DataFrameDataset(trainset, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\n",
        "test = DataFrameDataset(testset, target_column=target_column, dtype_x=dtype_x, dtype_y=dtype_y)\n",
        "n_samples = len(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# add the dataset to the fun_control\n",
        "fun_control.update({\"data\": train_df, # full dataset,\n",
        "               \"train\": train,\n",
        "               \"test\": test,\n",
        "               \"n_samples\": n_samples,\n",
        "               \"target_column\": target_column})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "config: {'_L0': 6112, 'l1': 32768, 'dropout_prob': 0.7103122166156, 'lr_mult': 0.001, 'batch_size': 4, 'epochs': 64, 'k_folds': 1, 'patience': 64, 'optimizer': 'AdamW', 'sgd_momentum': 0.9}\n",
            "Epoch: 1\n",
            "Loss on hold-out set: 2.39726079185054\n",
            "Accuracy on hold-out set: 0.10377358490566038\n",
            "MAPK value on hold-out data: 0.19732704402515722\n",
            "Epoch: 2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# add the nn model to the fun_control dictionary\n",
        "fun_control = add_core_model_to_fun_control(core_model=Net_vbdp,\n",
        "                              fun_control=fun_control,\n",
        "                              hyper_dict=TorchHyperDict)\n",
        "# modify the hyperparameter levels\n",
        "fun_control = modify_hyper_parameter_bounds(fun_control, \"_L0\", bounds=[n_features, n_features])\n",
        "# fun_control = modify_hyper_parameter_bounds(fun_control, \"l1\", bounds=[3, 4])\n",
        "# fun_control = modify_hyper_parameter_bounds(fun_control, \"epochs\", bounds=[2, 9])\n",
        "fun_control = modify_hyper_parameter_bounds(fun_control, \"patience\", bounds=[2, 6])\n",
        "fun_control = modify_hyper_parameter_bounds(fun_control, \"lr_mult\", bounds=[1e-3, 1e-3])\n",
        "fun_control = modify_hyper_parameter_bounds(fun_control, \"sgd_momentum\", bounds=[0.9, 0.9])\n",
        "fun_control = modify_hyper_parameter_levels(fun_control, \"optimizer\",[\"Adam\", \"AdamW\", \"Adamax\", \"NAdam\"])\n",
        "# select metric and loss function\n",
        "# metric_torch = torchmetrics.Accuracy(task=\"multiclass\", num_classes=11)\n",
        "metric_torch = MAPK(k=3)\n",
        "loss_torch = CrossEntropyLoss()\n",
        "# loss_torch = NLLLoss()\n",
        "fun_control.update({\n",
        "               \"metric_torch\": metric_torch,\n",
        "               \"loss_function\": loss_torch,\n",
        "               \"device\": \"cpu\",\n",
        "               })\n",
        "# extract the variable types, names, and bounds\n",
        "var_type = get_var_type(fun_control)\n",
        "var_name = get_var_name(fun_control)\n",
        "fun_control.update({\"var_type\": var_type,\n",
        "                    \"var_name\": var_name})\n",
        "lower = get_bound_values(fun_control, \"lower\")\n",
        "upper = get_bound_values(fun_control, \"upper\")\n",
        "\n",
        "# get the default hyperparameters as array\n",
        "hyper_dict=TorchHyperDict().load()\n",
        "X_start = get_default_hyperparameters_as_array(fun_control, hyper_dict)\n",
        "\n",
        "# get the objective function\n",
        "fun = HyperTorch().fun_torch\n",
        "\n",
        "# initialize spot\n",
        "spot_tuner = spot.Spot(fun=fun,\n",
        "                   lower = lower,\n",
        "                   upper = upper,\n",
        "                   fun_evals = inf,\n",
        "                   fun_repeats = 1,\n",
        "                   max_time = 1,\n",
        "                   noise = False,\n",
        "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
        "                   var_type = var_type,\n",
        "                   var_name = var_name,\n",
        "                   infill_criterion = \"y\",\n",
        "                   n_points = 1,\n",
        "                   seed=123,\n",
        "                   log_level = 50,\n",
        "                   show_models= False,\n",
        "                   show_progress= True,\n",
        "                   fun_control = fun_control,\n",
        "                   design_control={\"init_size\": 5,\n",
        "                                   \"repeats\": 1},\n",
        "                   surrogate_control={\"noise\": True,\n",
        "                                      \"cod_type\": \"norm\",\n",
        "                                      \"min_theta\": -4,\n",
        "                                      \"max_theta\": 3,\n",
        "                                      \"n_theta\": len(var_name),\n",
        "                                      \"model_fun_evals\": 10_000,\n",
        "                                      \"log_level\": 50\n",
        "                                      })\n",
        "# run spot\n",
        "spot_tuner.run(X_start=X_start)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "spot_tuner.plot_progress()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "from spotPython.utils.eda import gen_design_table\n",
        "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "spot_tuner.plot_importance()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\n",
        "model_spot = get_one_core_model_from_X(X, fun_control)\n",
        "model_spot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.torch.mapk import MAPK\n",
        "metric_torch = MAPK(k=3)\n",
        "fun_control.update({\n",
        "               \"metric_torch\": metric_torch,               \n",
        "               })\n",
        "\n",
        "train_tuned(net=model_spot, train_dataset=train,\n",
        "        loss_function=fun_control[\"loss_function\"],\n",
        "        metric=fun_control[\"metric_torch\"],\n",
        "        shuffle=True,\n",
        "        device = \"cpu\",\n",
        "        path=None,\n",
        "        task=fun_control[\"task\"],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_tuned(net=model_spot, test_dataset=test,\n",
        "            shuffle=False,\n",
        "            loss_function=fun_control[\"loss_function\"],\n",
        "            metric=fun_control[\"metric_torch\"],\n",
        "            device = \"cpu\",\n",
        "            task=fun_control[\"task\"],)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-validated Evaluations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* This is the evaluation that will be used in the comparison (evaluatecv has to be updated before, to get metric vlaues!):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spotPython.torch.traintest import evaluate_cv\n",
        "# modify k-kolds:+\n",
        "setattr(model_spot, \"k_folds\",  10)\n",
        "evaluate_cv(net=model_spot, dataset=fun_control[\"data\"], loss_function=fun_control[\"loss_function\"], metric=fun_control[\"metric_torch\"], task=fun_control[\"task\"], writer=fun_control[\"writer\"], writerId=\"model_spot_cv\", device=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "spot_tuner.plot_important_hyperparameter_contour()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# close tensorbaoard writer\n",
        "if fun_control[\"writer\"] is not None:\n",
        "    fun_control[\"writer\"].close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
